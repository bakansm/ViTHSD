{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14781,"status":"ok","timestamp":1680189876911,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"-WnzuzDQOBHA","outputId":"d62e050b-f402-4534-effc-d187c6b0dc77"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (4.27.3)\n","Requirement already satisfied: filelock in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from transformers) (3.10.7)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from transformers) (2023.3.23)\n","Requirement already satisfied: requests in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: pyyaml>=5.1 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from transformers) (23.0)\n","Requirement already satisfied: tqdm>=4.27 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from transformers) (4.65.0)\n","Requirement already satisfied: numpy>=1.17 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from transformers) (1.24.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from requests->transformers) (3.0.1)\n","Requirement already satisfied: idna<4,>=2.5 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1LKiXFQkluLn"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/bakansm/.conda/envs/tensorflow/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pickle\n","import tensorflow as tf\n","from keras.utils import pad_sequences\n","from transformers import AutoTokenizer\n","import numpy as np\n","import pandas as pd\n","from transformers import TFAutoModel, TFXLMRobertaModel\n","\n","class TargetedHSD:\n","    def __init__(self, model_path = None, tokenizer_path = None):\n","        if not model_path:\n","            self.__model_path = '../saved_model/bigrulstmcnn_xlmr2.h5'\n","        else:\n","            self.__model_path = model_path\n","        if not tokenizer_path:\n","            self.__tokenizer_path = 'xlm-roberta-base'\n","        else:\n","            self.__tokenizer_path = tokenizer_path\n","        \n","        self._tokenizer = AutoTokenizer.from_pretrained(self.__tokenizer_path)\n","        self._model = tf.keras.models.load_model(self.__model_path, custom_objects={'TFXLMRobertaModel': TFXLMRobertaModel})\n","        self.result = None\n","        self.orginal_label = None\n","    \n","    def predict(self, text):\n","        # encoded_text = self._tokenizer.texts_to_sequences([text])\n","        # encoded_text = pad_sequences(encoded_text, maxlen=100, padding='post')\n","        encoded_text = np.array(self._tokenizer([text], max_length=100, padding='max_length', truncation=True)['input_ids'])\n","        encoded_text = {\n","            \"input_ids\": np.asarray(self._tokenizer([text], max_length=50, padding='max_length', truncation=True)['input_ids']),\n","            \"attention_mask\": np.asarray(self._tokenizer([text], max_length=50, padding='max_length', truncation=True)['attention_mask'])\n","        }\n","        pred = self._model.predict(encoded_text)\n","        pred = np.argmax(pred.reshape(-1, 5, 4), axis=-1)\n","\n","        # y_test_pred_new = []\n","        # for y in pred:\n","        #     lb = []\n","        #     for i in range(0, len(y)):\n","        #         if y[i] >= 0.5:\n","        #             lb.append(1)\n","        #         else:\n","        #             lb.append(0)\n","        #     y_test_pred_new.append(lb)\n","        self.orginal_label = pred[0]\n","    \n","    def return_label(self):\n","        true_labels = []\n","        TYPE = {\n","            1: \"clean\",\n","            2: \"offensive\",\n","            3: \"hate\"\n","        }\n","        LABEL = {\n","            0: \"individual\",\n","            1: \"groups\",\n","            2: \"religion\",\n","            3: \"race\",\n","            4: \"politics\"\n","        }\n","        # LABEL = [('individual', 1),\n","        #             ('individual', 2),\n","        #             ('individual', 3),\n","        #             ('groups', 1),\n","        #             ('groups', 2),\n","        #             ('groups', 3),\n","        #             ('religion/creed', 1),\n","        #             ('religion/creed', 2),\n","        #             ('religion/creed', 3),\n","        #             ('race/ethnicity', 1),\n","        #             ('race/ethnicity', 2),\n","        #             ('race/ethnicity', 3),\n","        #             ('politics', 1),\n","        #             ('politics', 2),\n","        #             ('politics', 3)\n","        #         ]\n","        print(self.orginal_label)\n","        for i in range(0, len(self.orginal_label)):\n","            if self.orginal_label[i] > 0:\n","                t = LABEL[i] + \"#\" + TYPE[int(self.orginal_label[i])]\n","                true_labels.append(t)\n","\n","        self.result = true_labels\n","        return true_labels "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"VvyTXxRSnj82"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-04-06 18:56:17.067633: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2023-04-06 18:56:17.067662: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n","2023-04-06 18:56:17.067689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (BakanPC): /proc/driver/nvidia/version does not exist\n","2023-04-06 18:56:17.067964: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-06 18:56:19.010574: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n","2023-04-06 18:56:20.331063: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n","2023-04-06 18:56:20.486646: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n","2023-04-06 18:56:36.135691: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n","2023-04-06 18:56:37.170331: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m TargetedHSD()\n","Cell \u001b[0;32mIn[4], line 21\u001b[0m, in \u001b[0;36mTargetedHSD.__init__\u001b[0;34m(self, model_path, tokenizer_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__tokenizer_path \u001b[39m=\u001b[39m tokenizer_path\n\u001b[1;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__tokenizer_path)\n\u001b[0;32m---> 21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__model_path, custom_objects\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mTFXLMRobertaModel\u001b[39;49m\u001b[39m'\u001b[39;49m: TFXLMRobertaModel})\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morginal_label \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/keras/saving/legacy/save.py:242\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[39mif\u001b[39;00m h5py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    238\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mFilepath looks like a hdf5 file but h5py is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mnot available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m filepath=\u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m             )\n\u001b[0;32m--> 242\u001b[0m         \u001b[39mreturn\u001b[39;00m hdf5_format\u001b[39m.\u001b[39;49mload_model_from_hdf5(\n\u001b[1;32m    243\u001b[0m             tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mgfile\u001b[39m.\u001b[39;49mGFile(filepath_str, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    244\u001b[0m             custom_objects,\n\u001b[1;32m    245\u001b[0m             \u001b[39mcompile\u001b[39;49m,\n\u001b[1;32m    246\u001b[0m         )\n\u001b[1;32m    247\u001b[0m \u001b[39melif\u001b[39;00m h5py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath, h5py\u001b[39m.\u001b[39mFile):\n\u001b[1;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m hdf5_format\u001b[39m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    249\u001b[0m         filepath, custom_objects, \u001b[39mcompile\u001b[39m\n\u001b[1;32m    250\u001b[0m     )\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/keras/saving/legacy/hdf5_format.py:246\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    242\u001b[0m optimizer_weight_values \u001b[39m=\u001b[39m (\n\u001b[1;32m    243\u001b[0m     load_optimizer_weights_from_hdf5_group(f)\n\u001b[1;32m    244\u001b[0m )\n\u001b[1;32m    245\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 246\u001b[0m     model\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mset_weights(optimizer_weight_values)\n\u001b[1;32m    247\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError in loading the saved optimizer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstate. As a result, your model is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstarting with a freshly initialized \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moptimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m     )\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:800\u001b[0m, in \u001b[0;36m_BaseOptimizer.set_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[39mif\u001b[39;00m variable\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m weight\u001b[39m.\u001b[39mshape:\n\u001b[1;32m    795\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    796\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOptimizer variable \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(variable)\u001b[39m}\u001b[39;00m\u001b[39m has shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(variable\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m not compatible with provided \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweight shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(weight\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    799\u001b[0m     )\n\u001b[0;32m--> 800\u001b[0m variable\u001b[39m.\u001b[39;49massign(weight)\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:927\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[39m# Note: not depending on the cached value here since this can be used to\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[39m# initialize the variable.\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[39mwith\u001b[39;00m _handle_graph(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle):\n\u001b[0;32m--> 927\u001b[0m   value_tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(value, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m    928\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape\u001b[39m.\u001b[39mis_compatible_with(value_tensor\u001b[39m.\u001b[39mshape):\n\u001b[1;32m    929\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1636\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1627\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1628\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1629\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1632\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1633\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1635\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1638\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1639\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n","File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/h5py/_hl/dataset.py:1073\u001b[0m, in \u001b[0;36mDataset.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[39mif\u001b[39;00m numpy\u001b[39m.\u001b[39mproduct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mulonglong) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1071\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1073\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_direct(arr)\n\u001b[1;32m   1074\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/h5py/_hl/dataset.py:1034\u001b[0m, in \u001b[0;36mDataset.read_direct\u001b[0;34m(self, dest, source_sel, dest_sel)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     dest_sel \u001b[39m=\u001b[39m sel\u001b[39m.\u001b[39mselect(dest\u001b[39m.\u001b[39mshape, dest_sel)\n\u001b[1;32m   1033\u001b[0m \u001b[39mfor\u001b[39;00m mspace \u001b[39min\u001b[39;00m dest_sel\u001b[39m.\u001b[39mbroadcast(source_sel\u001b[39m.\u001b[39marray_shape):\n\u001b[0;32m-> 1034\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mid\u001b[39m.\u001b[39;49mread(mspace, fspace, dest, dxpl\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dxpl)\n","File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/h5d.pyx:240\u001b[0m, in \u001b[0;36mh5py.h5d.DatasetID.read\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/_proxy.pyx:112\u001b[0m, in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/h5fd.pyx:165\u001b[0m, in \u001b[0;36mh5py.h5fd.H5FD_fileobj_read\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/.conda/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/lib/io/file_io.py:121\u001b[0m, in \u001b[0;36mFileIO.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m   length \u001b[39m=\u001b[39m n\n\u001b[0;32m--> 121\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_value(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_buf\u001b[39m.\u001b[39;49mread(length))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["cls = TargetedHSD()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","dev = "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHVEY--upfFw"},"outputs":[],"source":["# Nhap vao day\n","# text = \"Nếu mà ai thích tiền, thích hào quang thì cứ lên đây để nếm thử 4 chữ hào quang rực rỡ đi rồi biết nó là cái gì!\"\n","text = \"Bông lan trứng muối\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1680192366560,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"F-x9m3G8o92g","outputId":"96782649-e9ed-49f2-e0ce-82e0f7f665a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 499ms/step\n"]}],"source":["cls.predict(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":465,"status":"ok","timestamp":1680192367016,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"k0Q5r7bQn5Yd","outputId":"2ce1f8da-8044-43ed-8f2a-1a29cdd8ed87"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1 0 0 0 0]\n"]},{"data":{"text/plain":["['individual#clean']"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["cls.return_label()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}

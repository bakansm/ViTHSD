{"cells":[{"cell_type":"markdown","source":["# lib"],"metadata":{"id":"-NxuE4bujL3h"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24092,"status":"ok","timestamp":1680065427181,"user":{"displayName":"Khánh Huỳnh Bảo","userId":"14657699746977533825"},"user_tz":-420},"id":"No1dszktOseS","outputId":"a331b900-aeca-4e6a-dd48-a085894e7598"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ZpLka9vpSOv","executionInfo":{"status":"ok","timestamp":1679987763846,"user_tz":-420,"elapsed":10912,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"fc015f56-0a00-40f3-c79f-77803e39a7c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"]}]},{"cell_type":"code","source":["pip install vncorenlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GP6xxwQgpaU-","executionInfo":{"status":"ok","timestamp":1679979902154,"user_tz":-420,"elapsed":6390,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"febcc5e0-48ab-47d1-c110-33d4f6236992"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting vncorenlp\n","  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from vncorenlp) (2.27.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->vncorenlp) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->vncorenlp) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->vncorenlp) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->vncorenlp) (3.4)\n","Building wheels for collected packages: vncorenlp\n","  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645952 sha256=aadf09a087fc30e9a66ee282245e591a8dec06859d0961a05a5766eaf2b68ccd\n","  Stored in directory: /root/.cache/pip/wheels/b2/1d/ea/9b94491d097561e07095d0800f2c5350ab183b49fc61780a8b\n","Successfully built vncorenlp\n","Installing collected packages: vncorenlp\n","Successfully installed vncorenlp-1.0.3\n"]}]},{"cell_type":"code","source":["!mkdir -p vncorenlp/models/wordsegmenter\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","!mv VnCoreNLP-1.1.1.jar vncorenlp/\n","\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","!mv vi-vocab vncorenlp/models/wordsegmenter/\n","!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n","\n","\n","!mkdir -p vncorenlp/models/postagger\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger \n","!mv vi-tagger vncorenlp/models/postagger/\n","\n","\n","!mkdir -p vncorenlp/models/ner\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz \n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n","!mv vi-500brownclusters.xz vncorenlp/models/ner/\n","!mv vi-ner.xz vncorenlp/models/ner/\n","!mv vi-pretrainedembeddings.xz vncorenlp/models/ner/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1yrulpAcvDRn","executionInfo":{"status":"ok","timestamp":1679979911107,"user_tz":-420,"elapsed":8956,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"4ca5d5bf-8a14-429d-9595-82963bb48425"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-03-28 05:05:01--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 27412575 (26M) [application/octet-stream]\n","Saving to: ‘VnCoreNLP-1.1.1.jar’\n","\n","VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   168MB/s    in 0.2s    \n","\n","2023-03-28 05:05:03 (168 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n","\n","--2023-03-28 05:05:03--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 526544 (514K) [application/octet-stream]\n","Saving to: ‘vi-vocab’\n","\n","vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.03s   \n","\n","2023-03-28 05:05:03 (16.0 MB/s) - ‘vi-vocab’ saved [526544/526544]\n","\n","--2023-03-28 05:05:03--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 128508 (125K) [text/plain]\n","Saving to: ‘wordsegmenter.rdr’\n","\n","wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.01s   \n","\n","2023-03-28 05:05:03 (8.51 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n","\n","--2023-03-28 05:05:04--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/postagger/vi-tagger\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 29709468 (28M) [application/octet-stream]\n","Saving to: ‘vi-tagger’\n","\n","vi-tagger           100%[===================>]  28.33M   174MB/s    in 0.2s    \n","\n","2023-03-28 05:05:05 (174 MB/s) - ‘vi-tagger’ saved [29709468/29709468]\n","\n","--2023-03-28 05:05:05--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-500brownclusters.xz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5599844 (5.3M) [application/octet-stream]\n","Saving to: ‘vi-500brownclusters.xz’\n","\n","vi-500brownclusters 100%[===================>]   5.34M  --.-KB/s    in 0.05s   \n","\n","2023-03-28 05:05:06 (111 MB/s) - ‘vi-500brownclusters.xz’ saved [5599844/5599844]\n","\n","--2023-03-28 05:05:06--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-ner.xz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9956876 (9.5M) [application/octet-stream]\n","Saving to: ‘vi-ner.xz’\n","\n","vi-ner.xz           100%[===================>]   9.50M  --.-KB/s    in 0.06s   \n","\n","2023-03-28 05:05:07 (163 MB/s) - ‘vi-ner.xz’ saved [9956876/9956876]\n","\n","--2023-03-28 05:05:07--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/ner/vi-pretrainedembeddings.xz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 57313672 (55M) [application/octet-stream]\n","Saving to: ‘vi-pretrainedembeddings.xz’\n","\n","vi-pretrainedembedd 100%[===================>]  54.66M   289MB/s    in 0.2s    \n","\n","2023-03-28 05:05:09 (289 MB/s) - ‘vi-pretrainedembeddings.xz’ saved [57313672/57313672]\n","\n"]}]},{"cell_type":"code","source":["VNCORE_NLP = False\n","max_len = 50"],"metadata":{"id":"oQMdID4DSqgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from vncorenlp import VnCoreNLP\n","vncorenlp = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\") "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"pipR2PBpn0UN","executionInfo":{"status":"error","timestamp":1679991826323,"user_tz":-420,"elapsed":6,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"42d0e721-92d6-49f5-8448-0a7467e159bc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-cd320ef4af87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvncorenlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVnCoreNLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvncorenlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVnCoreNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vncorenlp/VnCoreNLP-1.1.1.jar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wseg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vncorenlp'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["def tokennize_vn(text):\n","    sentences = vncorenlp.tokenize(text)\n","    s = ''\n","    for t in sentences:\n","        s = s + ' '.join(t) + ' '\n","    return s"],"metadata":{"id":"dLTv438lS0Tb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HIdSiF-20XCz"},"source":["# Read data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P49FSZZUqIQ2"},"outputs":[],"source":["train_dataset = 'drive/MyDrive/CODE/ViTHSD/dataset/train.xlsx'\n","dev_dataset = 'drive/MyDrive/CODE/ViTHSD/dataset/dev.xlsx'\n","test_dataset = 'drive/MyDrive/CODE/ViTHSD/dataset/test.xlsx'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pksSFBGM0oQl"},"outputs":[],"source":["import pandas as pd\n","\n","train = pd.read_csv(train_dataset)\n","dev = pd.read_csv(dev_dataset)\n","test = pd.read_csv(test_dataset)"]},{"cell_type":"code","source":["len(train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sgJI4DqAnotI","executionInfo":{"status":"ok","timestamp":1679991838235,"user_tz":-420,"elapsed":7,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"7af75dd2-926f-4b80-deb1-098bfa5f4fa4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7000"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["len(dev)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zb3MJ3wwfJCS","executionInfo":{"status":"ok","timestamp":1679991838235,"user_tz":-420,"elapsed":6,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"3863ae3e-979a-4e37-8e84-1e50b0880064"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1201"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["len(test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQXYVMxIfKvX","executionInfo":{"status":"ok","timestamp":1679991838235,"user_tz":-420,"elapsed":5,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"614e928f-8f40-4679-efce-f8783b87fad4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1800"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"NnI9Ruf70bDh"},"source":["# Feature extraction  "]},{"cell_type":"markdown","metadata":{"id":"0ZpGFaLArwlw"},"source":["## Make data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmXFMNzUqhES"},"outputs":[],"source":["import itertools\n","\n","aspect = list(train.columns.values[2:])\n","sent = [1,2,3]\n","\n","itertools.product([aspect, sent])\n","\n","combined_label = [a for a in itertools.product(aspect, sent)]"]},{"cell_type":"code","source":["combined_label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3MPJuPWoJKw","executionInfo":{"status":"ok","timestamp":1679991865000,"user_tz":-420,"elapsed":5,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"a00560f9-604a-47c4-8d78-04f919940cdc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('individual', 1),\n"," ('individual', 2),\n"," ('individual', 3),\n"," ('groups', 1),\n"," ('groups', 2),\n"," ('groups', 3),\n"," ('religion/creed', 1),\n"," ('religion/creed', 2),\n"," ('religion/creed', 3),\n"," ('race/ethnicity', 1),\n"," ('race/ethnicity', 2),\n"," ('race/ethnicity', 3),\n"," ('politics', 1),\n"," ('politics', 2),\n"," ('politics', 3)]"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ty_yXoG91MFH"},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","\n","def make_data(data):\n","    content = []\n","    label = []\n","\n","    for i in range(0, len(data)):\n","        row = data.iloc[i]\n","        if VNCORE_NLP:\n","            content.append(tokennize_vn(str(row['content'])))\n","        else:\n","            content.append(row['content'])\n","        default_lab = np.zeros(len(combined_label), dtype=int)\n","        for j in range(0, len(combined_label)):\n","            l = combined_label[j]\n","            if row[l[0]] == l[1]:\n","                default_lab[j] = 1\n","        label.append(default_lab)\n","\n","    return pd.DataFrame({\n","        'Content': content,\n","        'Labels': label\n","    })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_laKIWW1z1qt"},"outputs":[],"source":["train_data = make_data(train)\n","dev_data = make_data(dev)\n","test_data = make_data(test)"]},{"cell_type":"code","source":["test_data[1:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sh7xl8mwoFnq","executionInfo":{"status":"ok","timestamp":1679991866791,"user_tz":-420,"elapsed":10,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"7d88bd9e-5121-416c-e741-e9d1372964ec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             Content  \\\n","1  Đúng là ông bà có câu nhất lé quả không sai 🤔🤔...   \n","2  đéo hiểu sao 1 thằng đui và không có não mà lê...   \n","3                                    Hiếu Bùi sủa dơ   \n","4  A vẫn chăm chỉ nayd a vẫn cố găng cho đến khi ...   \n","5  Thái Lan hèn thật, người ta là đội ngũ tiên ph...   \n","6                     Chau oi bn la, con cua, cô  hả   \n","7                               Sao k có đoạn này ta   \n","8  Về chính trị, chúng tuyệt đối không cho nhân d...   \n","9  Gọi mấy thằng CA là phải dí thẳng tiền...   \n","\n","                                          Labels  \n","1  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","2  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","3  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","4  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","5  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]  \n","6  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","7  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","8  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n","9  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "],"text/html":["\n","  <div id=\"df-6a1fe212-64ce-463c-9783-cf6c89267771\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Content</th>\n","      <th>Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>Đúng là ông bà có câu nhất lé quả không sai 🤔🤔...</td>\n","      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>đéo hiểu sao 1 thằng đui và không có não mà lê...</td>\n","      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Hiếu Bùi sủa dơ</td>\n","      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A vẫn chăm chỉ nayd a vẫn cố găng cho đến khi ...</td>\n","      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Thái Lan hèn thật, người ta là đội ngũ tiên ph...</td>\n","      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Chau oi bn la, con cua, cô  hả</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Sao k có đoạn này ta</td>\n","      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Về chính trị, chúng tuyệt đối không cho nhân d...</td>\n","      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Gọi mấy thằng CA là phải dí thẳng tiền...</td>\n","      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a1fe212-64ce-463c-9783-cf6c89267771')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6a1fe212-64ce-463c-9783-cf6c89267771 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6a1fe212-64ce-463c-9783-cf6c89267771');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"IHC0eOV_xgnm"},"source":["## Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KCvhkfsxh7_"},"outputs":[],"source":["from keras.utils import pad_sequences\n","from keras.utils import to_categorical\n","from keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_data['Content'])\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","def encoding(X, y):\n","    X = tokenizer.texts_to_sequences(X.values.astype(str))\n","    X = pad_sequences(X, maxlen=max_len, padding='post')\n","    y = np.asarray([np.asarray(row, dtype=float) for row in y], dtype=int)\n","    return (X,y)"]},{"cell_type":"code","source":["# import pickle\n","\n","# # saving\n","# with open('drive/MyDrive/CODE/ViTHSD/model/tokenizer1.pickle', 'wb') as handle:\n","#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"km10a4n0cA7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vW7ZhnBfz-PK"},"outputs":[],"source":["train_features = encoding(train_data['Content'], train_data['Labels'])\n","dev_features = encoding(dev_data['Content'], dev_data['Labels'])\n","test_features = encoding(test_data['Content'], test_data['Labels'])"]},{"cell_type":"markdown","metadata":{"id":"QSK00T-SzDf6"},"source":["## Evaluation metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1b7h8G_UzGQJ"},"outputs":[],"source":["# Evaluation metric\n","import sys\n","import os\n","import os.path\n","from scipy.stats import sem\n","import numpy as np\n","from ast import literal_eval\n","import tensorflow as tf\n","\n","def em_score(y_true, y_pred):\n","    MR = np.all(y_pred == y_true, axis=1).mean()\n","    return MR\n","\n","def accuracy_score(y_true, y_pred):\n","    temp = 0\n","    for i in range(0, len(y_true)):\n","        temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n","    return temp / len(y_true)\n","\n"," \n","def f1_score(y_true, y_pred):\n","    temp = 0\n","    for i in range(len(y_true)):\n","        if (sum(y_true[i]) == 0) and (sum(y_pred[i]) == 0):\n","            continue\n","        temp+= (2*sum(np.logical_and(y_true[i], y_pred[i]))) / (sum(y_true[i])+sum(y_pred[i]))\n","    return temp/ len(y_true)"]},{"cell_type":"markdown","metadata":{"id":"kvrZ_gx0LvTB"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"xz13lEmoLvTC"},"source":["## Pre-trained Embedding "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":114364,"status":"ok","timestamp":1679900735276,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"636927c6-d635-4054-9896-8cb73e5beeb1","id":"L3Uafr9LLvTC"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 2000000 word vectors.\n"]}],"source":["import numpy as np\n","\n","EMBEDING_DIM = 300\n","NUM_LABEL = 15\n","EMBEDDING = 'drive/My Drive/CODE/ViTHSD/cc.vi.300.vec'\n","MAX_FEATURE = 10000\n","\n","embeddings_index = dict()\n","f = open(EMBEDDING)\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('Loaded %s word vectors.' % len(embeddings_index))\n","\n","\n","embedding_matrix = np.zeros((vocab_size, EMBEDING_DIM))\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"markdown","metadata":{"id":"EtQp3D4ZLvTE"},"source":["## Train "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2356,"status":"ok","timestamp":1679900737624,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ef748e8-c3f5-4315-9058-0b2f92969c38","id":"5EdEO6HSLvTE"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 100)]        0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 100, 300)     2132400     ['input_1[0][0]']                \n","                                                                                                  \n"," spatial_dropout1d (SpatialDrop  (None, 100, 300)    0           ['embedding[0][0]']              \n"," out1D)                                                                                           \n","                                                                                                  \n"," bidirectional (Bidirectional)  (None, 100, 200)     241200      ['spatial_dropout1d[0][0]']      \n","                                                                                                  \n"," bidirectional_1 (Bidirectional  (None, 100, 200)    320800      ['spatial_dropout1d[0][0]']      \n"," )                                                                                                \n","                                                                                                  \n"," conv1d (Conv1D)                (None, 99, 50)       20050       ['bidirectional[0][0]']          \n","                                                                                                  \n"," conv1d_1 (Conv1D)              (None, 99, 50)       20050       ['bidirectional_1[0][0]']        \n","                                                                                                  \n"," global_average_pooling1d (Glob  (None, 50)          0           ['conv1d[0][0]']                 \n"," alAveragePooling1D)                                                                              \n","                                                                                                  \n"," global_max_pooling1d (GlobalMa  (None, 50)          0           ['conv1d[0][0]']                 \n"," xPooling1D)                                                                                      \n","                                                                                                  \n"," global_average_pooling1d_1 (Gl  (None, 50)          0           ['conv1d_1[0][0]']               \n"," obalAveragePooling1D)                                                                            \n","                                                                                                  \n"," global_max_pooling1d_1 (Global  (None, 50)          0           ['conv1d_1[0][0]']               \n"," MaxPooling1D)                                                                                    \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 200)          0           ['global_average_pooling1d[0][0]'\n","                                                                 , 'global_max_pooling1d[0][0]',  \n","                                                                  'global_average_pooling1d_1[0][0\n","                                                                 ]',                              \n","                                                                  'global_max_pooling1d_1[0][0]'] \n","                                                                                                  \n"," dense (Dense)                  (None, 15)           3015        ['concatenate[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 2,737,515\n","Trainable params: 2,737,515\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["# Bi-GRU-LSTM-CNN\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from keras.models import Sequential, Model\n","\n","from keras.layers import Dense, Concatenate, Bidirectional, GRU, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Dropout, concatenate\n","from keras.layers import Flatten, LSTM, Input\n","from tensorflow.keras.layers import Embedding\n","\n","units = 100\n","\n","input = Input(shape = (max_len,))\n","emb = Embedding(vocab_size, EMBEDING_DIM, weights=[embedding_matrix], input_length=max_len, trainable=True)(input)\n","x1 = SpatialDropout1D(0.2)(emb)\n","\n","x = Bidirectional(GRU(units, return_sequences = True))(x1)\n","x = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n","    \n","y = Bidirectional(LSTM(units, return_sequences = True))(x1)\n","y = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n","    \n","avg_pool1 = GlobalAveragePooling1D()(x)\n","max_pool1 = GlobalMaxPooling1D()(x)\n","    \n","avg_pool2 = GlobalAveragePooling1D()(y)\n","max_pool2 = GlobalMaxPooling1D()(y)\n","    \n","    \n","x = Concatenate(axis=-1)([avg_pool1, max_pool1, avg_pool2, max_pool2])\n","out = Dense(15, activation = \"sigmoid\")(x)\n","\n","model = Model(input, out)\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679900833147,"user_tz":-420,"elapsed":95534,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"bf174251-32d9-427d-b2c6-ad591152396a","id":"mGd1ipPrLvTG"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","55/55 [==============================] - 22s 199ms/step - loss: 0.3012 - acc: 0.4026 - val_loss: 0.2164 - val_acc: 0.5679\n","Epoch 2/30\n","55/55 [==============================] - 7s 132ms/step - loss: 0.2034 - acc: 0.5904 - val_loss: 0.1899 - val_acc: 0.6020\n","Epoch 3/30\n","55/55 [==============================] - 6s 105ms/step - loss: 0.1777 - acc: 0.6424 - val_loss: 0.1809 - val_acc: 0.6103\n","Epoch 4/30\n","55/55 [==============================] - 6s 105ms/step - loss: 0.1577 - acc: 0.6627 - val_loss: 0.1823 - val_acc: 0.6087\n","Epoch 5/30\n","55/55 [==============================] - 4s 78ms/step - loss: 0.1406 - acc: 0.6849 - val_loss: 0.1862 - val_acc: 0.5928\n","Epoch 6/30\n","55/55 [==============================] - 4s 66ms/step - loss: 0.1248 - acc: 0.6956 - val_loss: 0.1938 - val_acc: 0.5321\n","Epoch 7/30\n","55/55 [==============================] - 4s 69ms/step - loss: 0.1098 - acc: 0.7221 - val_loss: 0.2170 - val_acc: 0.5828\n","Epoch 8/30\n","55/55 [==============================] - 3s 50ms/step - loss: 0.0968 - acc: 0.7390 - val_loss: 0.2258 - val_acc: 0.5329\n","Epoch 9/30\n","55/55 [==============================] - 3s 47ms/step - loss: 0.0843 - acc: 0.7571 - val_loss: 0.2396 - val_acc: 0.5204\n","Epoch 10/30\n","55/55 [==============================] - 3s 47ms/step - loss: 0.0731 - acc: 0.7646 - val_loss: 0.2550 - val_acc: 0.5296\n","Epoch 11/30\n","55/55 [==============================] - 2s 39ms/step - loss: 0.0657 - acc: 0.7616 - val_loss: 0.2713 - val_acc: 0.5346\n","Epoch 12/30\n","55/55 [==============================] - 2s 36ms/step - loss: 0.0551 - acc: 0.7746 - val_loss: 0.2902 - val_acc: 0.5171\n","Epoch 13/30\n","55/55 [==============================] - 2s 29ms/step - loss: 0.0473 - acc: 0.7667 - val_loss: 0.3169 - val_acc: 0.5337\n","Epoch 14/30\n","55/55 [==============================] - 2s 28ms/step - loss: 0.0421 - acc: 0.7671 - val_loss: 0.3314 - val_acc: 0.5004\n","Epoch 15/30\n","55/55 [==============================] - 3s 47ms/step - loss: 0.0367 - acc: 0.7616 - val_loss: 0.3486 - val_acc: 0.5087\n","Epoch 16/30\n","55/55 [==============================] - 2s 41ms/step - loss: 0.0320 - acc: 0.7774 - val_loss: 0.3666 - val_acc: 0.4954\n","Epoch 17/30\n","55/55 [==============================] - 2s 32ms/step - loss: 0.0286 - acc: 0.7633 - val_loss: 0.3825 - val_acc: 0.4813\n","Epoch 18/30\n","55/55 [==============================] - 2s 38ms/step - loss: 0.0247 - acc: 0.7644 - val_loss: 0.3970 - val_acc: 0.4729\n","Epoch 19/30\n","55/55 [==============================] - 2s 31ms/step - loss: 0.0237 - acc: 0.7544 - val_loss: 0.4153 - val_acc: 0.4821\n","Epoch 20/30\n","55/55 [==============================] - 2s 38ms/step - loss: 0.0207 - acc: 0.7567 - val_loss: 0.4296 - val_acc: 0.4754\n","Epoch 21/30\n","55/55 [==============================] - 2s 31ms/step - loss: 0.0186 - acc: 0.7581 - val_loss: 0.4459 - val_acc: 0.4679\n","Epoch 22/30\n","55/55 [==============================] - 2s 29ms/step - loss: 0.0170 - acc: 0.7617 - val_loss: 0.4590 - val_acc: 0.4788\n","Epoch 23/30\n","55/55 [==============================] - 1s 22ms/step - loss: 0.0158 - acc: 0.7570 - val_loss: 0.4676 - val_acc: 0.4871\n","Epoch 24/30\n","55/55 [==============================] - 2s 31ms/step - loss: 0.0156 - acc: 0.7591 - val_loss: 0.4838 - val_acc: 0.4571\n","Epoch 25/30\n","55/55 [==============================] - 2s 31ms/step - loss: 0.0149 - acc: 0.7610 - val_loss: 0.4902 - val_acc: 0.4363\n","Epoch 26/30\n","55/55 [==============================] - 2s 29ms/step - loss: 0.0126 - acc: 0.7534 - val_loss: 0.5179 - val_acc: 0.4571\n","Epoch 27/30\n","55/55 [==============================] - 2s 29ms/step - loss: 0.0120 - acc: 0.7483 - val_loss: 0.5269 - val_acc: 0.4629\n","Epoch 28/30\n","55/55 [==============================] - 2s 33ms/step - loss: 0.0118 - acc: 0.7439 - val_loss: 0.5261 - val_acc: 0.4546\n","Epoch 29/30\n","55/55 [==============================] - 2s 29ms/step - loss: 0.0108 - acc: 0.7526 - val_loss: 0.5301 - val_acc: 0.4446\n","Epoch 30/30\n","55/55 [==============================] - 2s 29ms/step - loss: 0.0109 - acc: 0.7481 - val_loss: 0.5476 - val_acc: 0.4688\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6b580ea880>"]},"metadata":{},"execution_count":16}],"source":["model.fit(train_features[0].astype(np.float32), train_features[1], validation_data=(dev_features[0].astype(np.float32), dev_features[1]), \n","          batch_size=128, epochs=30, verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"RvLezpemHj9f"},"source":["## Eval"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2281,"status":"ok","timestamp":1679900835425,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"pUuDyLLYHlfZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0d544f49-6c3e-43f6-89e0-7bc20e264cdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["57/57 [==============================] - 2s 9ms/step\n"]}],"source":["y_test_pred = model.predict(test_features[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psDLv9H_ITJC"},"outputs":[],"source":["y_test_pred_new = []\n","\n","for y in y_test_pred:\n","    lb = []\n","    for i in range(0, len(y)):\n","        if y[i] >= 0.5:\n","            lb.append(1)\n","        else:\n","            lb.append(0)\n","    y_test_pred_new.append(lb)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679900836193,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"id":"oWCoOxh5IWN8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"015cd4f4-7dcc-44f6-b24c-6395997d6805"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["39.910052910052826"]},"metadata":{},"execution_count":19}],"source":["f1_score(test_features[1], y_test_pred_new)*100"]},{"cell_type":"markdown","metadata":{"id":"c1DCdUWGLpHX"},"source":["# Model - pre-trained transformers"]},{"cell_type":"markdown","metadata":{"id":"G4kszGP1LpHY"},"source":["## Pre-trained Embedding "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1958,"status":"ok","timestamp":1679992107192,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"048078e0-83e1-41a1-9e8a-ff143d44d029","id":"-Qr3j_oRLpHZ"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at NlpHUST/vibert4news-base-cased were not used when initializing IBertModel: ['bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'cls.predictions.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.dense.bias']\n","- This IS expected if you are initializing IBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing IBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of IBertModel were not initialized from the model checkpoint at NlpHUST/vibert4news-base-cased and are newly initialized: ['encoder.layer.3.intermediate.output_activation.act_scaling_factor', 'encoder.layer.3.attention.self.query_activation.x_max', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.output.output_activation.act_scaling_factor', 'encoder.layer.2.intermediate.output_activation.x_min', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.pre_output_act.x_min', 'encoder.layer.9.attention.output.output_activation.x_max', 'encoder.layer.2.attention.self.value_activation.x_max', 'encoder.layer.1.attention.output.LayerNorm.activation.x_min', 'encoder.layer.1.output.LayerNorm.activation.x_max', 'encoder.layer.11.pre_intermediate_act.x_min', 'encoder.layer.5.attention.self.query_activation.x_max', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.self.output_activation.x_max', 'encoder.layer.8.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.7.attention.self.query.bias_integer', 'encoder.layer.3.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.3.attention.self.value.bias_integer', 'encoder.layer.4.pre_output_act.act_scaling_factor', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.key_activation.x_max', 'encoder.layer.3.output.output_activation.act_scaling_factor', 'encoder.layer.9.output.LayerNorm.shift', 'encoder.layer.4.attention.output.output_activation.x_max', 'encoder.layer.1.attention.self.value.fc_scaling_factor', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.output_activation.x_max', 'encoder.layer.2.pre_intermediate_act.act_scaling_factor', 'encoder.layer.6.attention.output.ln_input_act.x_min', 'encoder.layer.3.intermediate.output_activation.x_min', 'encoder.layer.0.attention.self.output_activation.x_max', 'encoder.layer.11.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.4.attention.output.output_activation.act_scaling_factor', 'encoder.layer.7.output.LayerNorm.activation.x_min', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight_integer', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.1.intermediate.dense.bias_integer', 'encoder.layer.11.attention.self.output_activation.x_max', 'encoder.layer.0.attention.self.query_activation.x_min', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias_integer', 'encoder.layer.6.attention.output.LayerNorm.activation.x_min', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.pre_intermediate_act.x_max', 'encoder.layer.4.output.output_activation.x_max', 'encoder.layer.8.attention.self.query.fc_scaling_factor', 'encoder.layer.0.output.ln_input_act.x_max', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value_activation.x_max', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.pre_output_act.x_max', 'encoder.layer.6.pre_output_act.x_max', 'encoder.layer.5.attention.output.dense.bias_integer', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.attention.self.query.fc_scaling_factor', 'encoder.layer.9.attention.self.key_activation.act_scaling_factor', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.output.ln_input_act.x_max', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.attention.self.query_activation.act_scaling_factor', 'encoder.layer.6.output.dense.bias_integer', 'encoder.layer.1.output.dense.fc_scaling_factor', 'encoder.layer.3.attention.output.LayerNorm.activation.x_min', 'encoder.layer.6.output.dense.weight_integer', 'encoder.layer.4.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias_integer', 'encoder.layer.11.attention.self.query.fc_scaling_factor', 'encoder.layer.6.output.output_activation.x_min', 'encoder.layer.7.attention.self.key.bias_integer', 'encoder.layer.10.intermediate.dense.fc_scaling_factor', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.7.attention.self.softmax.act.x_max', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value_activation.act_scaling_factor', 'encoder.layer.5.attention.output.ln_input_act.x_max', 'encoder.layer.5.output.output_activation.x_min', 'encoder.layer.9.intermediate.output_activation.x_max', 'encoder.layer.11.attention.output.ln_input_act.x_max', 'encoder.layer.11.pre_output_act.x_max', 'encoder.layer.7.attention.output.LayerNorm.shift', 'encoder.layer.9.attention.self.value.bias_integer', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.fc_scaling_factor', 'encoder.layer.6.attention.self.key_activation.act_scaling_factor', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.fc_scaling_factor', 'encoder.layer.9.intermediate.dense.bias_integer', 'encoder.layer.8.output.dense.fc_scaling_factor', 'encoder.layer.9.attention.self.value_activation.act_scaling_factor', 'encoder.layer.10.pre_output_act.act_scaling_factor', 'encoder.layer.1.attention.self.query.bias_integer', 'encoder.layer.1.attention.output.ln_input_act.x_min', 'encoder.layer.3.attention.self.value.weight_integer', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.2.attention.output.output_activation.x_max', 'encoder.layer.5.output.dense.fc_scaling_factor', 'encoder.layer.8.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.8.output.LayerNorm.shift', 'encoder.layer.0.intermediate.output_activation.act_scaling_factor', 'encoder.layer.6.attention.output.output_activation.x_min', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.output.dense.fc_scaling_factor', 'encoder.layer.6.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.7.attention.output.dense.weight_integer', 'encoder.layer.7.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.4.attention.output.dense.bias_integer', 'encoder.layer.3.attention.self.value_activation.x_max', 'encoder.layer.8.output.dense.weight_integer', 'encoder.layer.5.attention.output.LayerNorm.activation.x_max', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.pre_intermediate_act.act_scaling_factor', 'encoder.layer.9.attention.self.value.fc_scaling_factor', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.3.pre_output_act.x_min', 'encoder.layer.4.intermediate.output_activation.act_scaling_factor', 'embeddings.LayerNorm.activation.x_max', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias_integer', 'encoder.layer.11.attention.self.value.bias_integer', 'encoder.layer.1.pre_intermediate_act.x_max', 'encoder.layer.6.attention.output.LayerNorm.shift', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.output.ln_input_act.x_max', 'encoder.layer.11.intermediate.dense.fc_scaling_factor', 'encoder.layer.9.attention.output.dense.weight_integer', 'encoder.layer.3.output.LayerNorm.activation.x_min', 'encoder.layer.10.intermediate.output_activation.x_max', 'encoder.layer.9.pre_intermediate_act.x_max', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key_activation.act_scaling_factor', 'encoder.layer.11.attention.self.output_activation.x_min', 'encoder.layer.11.output.LayerNorm.activation.x_max', 'encoder.layer.8.intermediate.dense.weight_integer', 'embeddings.word_embeddings.weight_scaling_factor', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.output.ln_input_act.act_scaling_factor', 'encoder.layer.3.output.ln_input_act.x_max', 'encoder.layer.8.output.output_activation.x_max', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.pre_output_act.act_scaling_factor', 'encoder.layer.0.attention.self.query_activation.x_max', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight_integer', 'encoder.layer.5.attention.output.output_activation.x_max', 'encoder.layer.5.intermediate.dense.fc_scaling_factor', 'encoder.layer.5.pre_intermediate_act.x_min', 'encoder.layer.5.intermediate.output_activation.x_min', 'encoder.layer.0.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.9.attention.output.dense.fc_scaling_factor', 'encoder.layer.10.attention.output.ln_input_act.x_min', 'encoder.layer.3.attention.self.query.fc_scaling_factor', 'encoder.layer.2.attention.self.key_activation.x_max', 'encoder.layer.5.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.6.intermediate.output_activation.act_scaling_factor', 'encoder.layer.9.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.11.intermediate.dense.bias_integer', 'encoder.layer.0.output.output_activation.x_min', 'encoder.layer.2.output.LayerNorm.activation.x_max', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.output.dense.weight_integer', 'encoder.layer.11.attention.self.key_activation.x_min', 'encoder.layer.4.output.dense.weight_integer', 'encoder.layer.0.intermediate.dense.weight_integer', 'encoder.layer.8.intermediate.output_activation.act_scaling_factor', 'encoder.layer.10.attention.self.value.bias_integer', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.value_activation.x_max', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.weight_integer', 'encoder.layer.10.output.dense.bias', 'encoder.layer.2.attention.output.ln_input_act.x_min', 'encoder.layer.8.attention.output.LayerNorm.activation.x_min', 'encoder.layer.11.attention.self.value.fc_scaling_factor', 'encoder.layer.3.intermediate.output_activation.x_max', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.fc_scaling_factor', 'encoder.layer.9.output.ln_input_act.x_min', 'encoder.layer.8.attention.self.value.fc_scaling_factor', 'encoder.layer.10.attention.self.key.bias_integer', 'encoder.layer.10.output.ln_input_act.x_max', 'encoder.layer.9.output.dense.bias_integer', 'encoder.layer.4.attention.output.dense.weight_integer', 'encoder.layer.3.output.output_activation.x_max', 'encoder.layer.6.output.ln_input_act.x_max', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.fc_scaling_factor', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.self.value.bias_integer', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.10.attention.output.output_activation.x_max', 'encoder.layer.0.attention.output.LayerNorm.shift', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.attention.self.output_activation.act_scaling_factor', 'encoder.layer.2.output.LayerNorm.activation.x_min', 'encoder.layer.9.attention.self.value_activation.x_min', 'encoder.layer.11.output.dense.bias', 'encoder.layer.3.attention.self.softmax.act.x_min', 'encoder.layer.7.attention.self.key.weight_integer', 'encoder.layer.3.attention.self.output_activation.x_min', 'encoder.layer.9.intermediate.output_activation.x_min', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.output_activation.act_scaling_factor', 'encoder.layer.0.output.LayerNorm.activation.x_max', 'encoder.layer.1.output.dense.weight_integer', 'encoder.layer.7.attention.output.LayerNorm.activation.x_max', 'encoder.layer.6.attention.output.output_activation.act_scaling_factor', 'encoder.layer.8.attention.output.LayerNorm.activation.x_max', 'encoder.layer.11.output.dense.bias_integer', 'encoder.layer.10.output.dense.bias_integer', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight_integer', 'encoder.layer.11.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.8.attention.self.value_activation.x_max', 'encoder.layer.11.output.output_activation.act_scaling_factor', 'encoder.layer.3.attention.self.value_activation.act_scaling_factor', 'encoder.layer.9.attention.self.key.fc_scaling_factor', 'encoder.layer.0.attention.self.key.weight_integer', 'encoder.layer.1.attention.self.query_activation.x_max', 'encoder.layer.1.pre_output_act.x_min', 'encoder.layer.3.attention.self.key_activation.act_scaling_factor', 'encoder.layer.3.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.8.attention.self.output_activation.x_max', 'encoder.layer.4.attention.output.ln_input_act.x_max', 'encoder.layer.1.intermediate.dense.fc_scaling_factor', 'encoder.layer.11.attention.self.value.weight_integer', 'encoder.layer.3.output.ln_input_act.x_min', 'encoder.layer.5.output.ln_input_act.x_min', 'encoder.layer.6.intermediate.dense.bias', 'embeddings.embeddings_act2.x_max', 'encoder.layer.7.output.dense.weight_integer', 'encoder.layer.5.output.ln_input_act.x_max', 'encoder.layer.5.attention.output.LayerNorm.activation.x_min', 'encoder.layer.7.attention.self.query.weight_integer', 'encoder.layer.4.output.output_activation.x_min', 'encoder.layer.10.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.11.attention.output.output_activation.x_min', 'embeddings.embeddings_act1.x_min', 'encoder.layer.5.attention.self.key_activation.act_scaling_factor', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.3.attention.self.key.fc_scaling_factor', 'encoder.layer.6.attention.self.key_activation.x_min', 'encoder.layer.5.output.LayerNorm.shift', 'encoder.layer.7.pre_intermediate_act.x_min', 'encoder.layer.6.attention.self.softmax.act.x_max', 'encoder.layer.4.attention.self.value_activation.x_min', 'encoder.layer.3.attention.output.LayerNorm.shift', 'encoder.layer.6.pre_intermediate_act.x_max', 'encoder.layer.2.attention.output.dense.weight_integer', 'encoder.layer.8.attention.output.dense.bias_integer', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.attention.self.value_activation.x_max', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.output.ln_input_act.act_scaling_factor', 'encoder.layer.10.output.output_activation.x_min', 'encoder.layer.2.output.ln_input_act.x_min', 'embeddings.LayerNorm.shift', 'encoder.layer.2.attention.self.output_activation.act_scaling_factor', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.intermediate.output_activation.x_max', 'embeddings.embeddings_act1.act_scaling_factor', 'encoder.layer.3.attention.output.output_activation.act_scaling_factor', 'encoder.layer.5.pre_intermediate_act.act_scaling_factor', 'encoder.layer.4.attention.self.value.bias_integer', 'encoder.layer.10.attention.self.value_activation.x_min', 'encoder.layer.6.attention.self.output_activation.x_max', 'encoder.layer.0.pre_output_act.x_max', 'encoder.layer.11.attention.self.query_activation.x_min', 'encoder.layer.5.attention.self.key.weight_integer', 'encoder.layer.5.attention.output.dense.weight_integer', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.self.output_activation.act_scaling_factor', 'encoder.layer.4.intermediate.dense.weight_integer', 'encoder.layer.10.output.ln_input_act.x_min', 'encoder.layer.10.attention.output.LayerNorm.activation.x_min', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.activation.x_max', 'encoder.layer.3.output.ln_input_act.act_scaling_factor', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.pre_intermediate_act.x_max', 'encoder.layer.6.pre_output_act.act_scaling_factor', 'encoder.layer.8.output.LayerNorm.activation.act_scaling_factor', 'embeddings.LayerNorm.activation.act_scaling_factor', 'encoder.layer.8.attention.self.softmax.act.x_min', 'encoder.layer.5.attention.self.key_activation.x_max', 'encoder.layer.4.intermediate.output_activation.x_min', 'encoder.layer.6.output.LayerNorm.activation.x_max', 'encoder.layer.11.intermediate.dense.weight_integer', 'encoder.layer.2.output.dense.bias_integer', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.shift', 'encoder.layer.3.attention.self.key.weight_integer', 'encoder.layer.0.attention.output.ln_input_act.x_max', 'encoder.layer.6.intermediate.output_activation.x_min', 'encoder.layer.8.output.LayerNorm.activation.x_max', 'encoder.layer.3.pre_intermediate_act.x_min', 'encoder.layer.7.intermediate.dense.fc_scaling_factor', 'encoder.layer.6.output.LayerNorm.activation.x_min', 'encoder.layer.7.attention.self.value.weight_integer', 'encoder.layer.9.attention.output.ln_input_act.x_min', 'encoder.layer.1.attention.self.key_activation.x_min', 'encoder.layer.3.attention.output.ln_input_act.x_min', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.output.output_activation.x_min', 'encoder.layer.9.pre_intermediate_act.act_scaling_factor', 'encoder.layer.10.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.7.output.LayerNorm.shift', 'encoder.layer.6.attention.self.value_activation.x_min', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.activation.x_min', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.10.output.dense.fc_scaling_factor', 'encoder.layer.1.attention.output.LayerNorm.activation.x_max', 'encoder.layer.2.intermediate.dense.weight_integer', 'encoder.layer.6.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.10.attention.self.value_activation.x_max', 'encoder.layer.5.attention.self.value.weight_integer', 'encoder.layer.0.attention.self.value_activation.x_min', 'encoder.layer.3.output.dense.weight_integer', 'encoder.layer.9.output.output_activation.x_max', 'encoder.layer.1.attention.self.output_activation.x_max', 'encoder.layer.2.pre_intermediate_act.x_max', 'encoder.layer.6.pre_intermediate_act.x_min', 'encoder.layer.4.output.ln_input_act.act_scaling_factor', 'embeddings.word_embeddings.weight_integer', 'encoder.layer.1.output.ln_input_act.x_max', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.attention.self.key_activation.x_min', 'encoder.layer.9.attention.self.query_activation.act_scaling_factor', 'encoder.layer.8.attention.self.query.weight_integer', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.output.LayerNorm.shift', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.activation.x_max', 'encoder.layer.6.output.ln_input_act.act_scaling_factor', 'encoder.layer.8.output.ln_input_act.x_min', 'encoder.layer.3.attention.self.softmax.act.x_max', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.fc_scaling_factor', 'encoder.layer.6.attention.output.dense.fc_scaling_factor', 'encoder.layer.7.intermediate.output_activation.x_min', 'encoder.layer.5.pre_intermediate_act.x_max', 'encoder.layer.11.attention.self.softmax.act.x_max', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.pre_intermediate_act.x_min', 'encoder.layer.11.intermediate.output_activation.x_min', 'encoder.layer.7.attention.output.ln_input_act.x_max', 'embeddings.output_activation.x_min', 'encoder.layer.6.attention.output.output_activation.x_max', 'encoder.layer.11.pre_intermediate_act.act_scaling_factor', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.output.output_activation.act_scaling_factor', 'encoder.layer.8.attention.self.key_activation.x_min', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.self.query_activation.x_max', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.output.ln_input_act.x_max', 'encoder.layer.11.output.output_activation.x_max', 'encoder.layer.6.attention.self.query_activation.act_scaling_factor', 'encoder.layer.1.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.fc_scaling_factor', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.5.attention.self.value_activation.act_scaling_factor', 'encoder.layer.7.attention.self.output_activation.act_scaling_factor', 'encoder.layer.8.attention.output.output_activation.x_min', 'encoder.layer.7.attention.self.key_activation.x_max', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.7.attention.self.value.fc_scaling_factor', 'encoder.layer.11.attention.output.output_activation.x_max', 'encoder.layer.3.pre_intermediate_act.x_max', 'encoder.layer.2.attention.self.value.weight_integer', 'encoder.layer.1.attention.output.LayerNorm.shift', 'encoder.layer.5.output.LayerNorm.activation.x_min', 'encoder.layer.9.attention.self.key.weight_integer', 'encoder.layer.5.attention.self.key.bias_integer', 'encoder.layer.3.attention.output.ln_input_act.x_max', 'encoder.layer.6.attention.self.query_activation.x_max', 'encoder.layer.8.attention.self.value_activation.x_min', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight_integer', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.9.pre_output_act.x_min', 'encoder.layer.2.attention.self.query.weight_integer', 'encoder.layer.8.attention.self.query.bias_integer', 'encoder.layer.2.attention.self.query_activation.act_scaling_factor', 'encoder.layer.0.output.output_activation.x_max', 'encoder.layer.6.attention.self.value.weight_integer', 'encoder.layer.4.attention.output.LayerNorm.shift', 'encoder.layer.8.attention.output.ln_input_act.x_max', 'encoder.layer.3.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.9.output.output_activation.act_scaling_factor', 'encoder.layer.10.attention.self.query.bias_integer', 'encoder.layer.2.attention.self.query_activation.x_max', 'encoder.layer.7.output.dense.bias', 'encoder.layer.1.attention.self.key_activation.x_max', 'encoder.layer.5.attention.output.LayerNorm.shift', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.pre_intermediate_act.x_min', 'encoder.layer.11.attention.self.query_activation.act_scaling_factor', 'encoder.layer.1.attention.self.key.weight_integer', 'encoder.layer.10.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.0.intermediate.dense.bias_integer', 'encoder.layer.4.attention.self.value.fc_scaling_factor', 'encoder.layer.6.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.2.attention.output.ln_input_act.x_max', 'encoder.layer.9.attention.self.query.fc_scaling_factor', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.activation.x_max', 'encoder.layer.1.attention.self.query.weight_integer', 'encoder.layer.4.intermediate.output_activation.x_max', 'embeddings.token_type_embeddings.weight_scaling_factor', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'embeddings.embeddings_act1.x_max', 'encoder.layer.5.attention.output.dense.fc_scaling_factor', 'encoder.layer.4.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.8.intermediate.output_activation.x_min', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.7.pre_intermediate_act.act_scaling_factor', 'encoder.layer.6.attention.self.key.bias_integer', 'encoder.layer.1.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.7.output.output_activation.x_max', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.ln_input_act.x_max', 'encoder.layer.2.output.output_activation.x_min', 'encoder.layer.9.attention.self.key.bias_integer', 'encoder.layer.9.output.output_activation.x_min', 'encoder.layer.10.attention.self.value_activation.act_scaling_factor', 'encoder.layer.3.pre_output_act.x_max', 'encoder.layer.7.attention.self.output_activation.x_min', 'encoder.layer.9.attention.output.dense.bias_integer', 'encoder.layer.1.attention.self.value.weight_integer', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.activation.x_min', 'encoder.layer.8.attention.self.key.bias_integer', 'encoder.layer.9.attention.self.output_activation.x_min', 'encoder.layer.1.output.dense.bias_integer', 'encoder.layer.0.attention.output.output_activation.x_min', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.self.output_activation.act_scaling_factor', 'encoder.layer.8.attention.self.output_activation.act_scaling_factor', 'encoder.layer.10.attention.self.query_activation.act_scaling_factor', 'encoder.layer.6.output.ln_input_act.x_min', 'encoder.layer.11.output.ln_input_act.x_max', 'encoder.layer.8.attention.self.key.weight_integer', 'encoder.layer.11.pre_output_act.act_scaling_factor', 'encoder.layer.6.intermediate.dense.fc_scaling_factor', 'encoder.layer.3.attention.self.value.fc_scaling_factor', 'encoder.layer.9.attention.self.query_activation.x_min', 'encoder.layer.5.attention.self.value.fc_scaling_factor', 'encoder.layer.7.attention.output.dense.bias_integer', 'encoder.layer.8.attention.output.output_activation.act_scaling_factor', 'encoder.layer.5.attention.self.query.fc_scaling_factor', 'encoder.layer.10.attention.self.softmax.act.x_min', 'encoder.layer.3.intermediate.dense.bias_integer', 'encoder.layer.3.attention.self.key_activation.x_max', 'encoder.layer.5.attention.self.query_activation.act_scaling_factor', 'encoder.layer.6.attention.self.key_activation.x_max', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.activation.x_max', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.output.ln_input_act.act_scaling_factor', 'encoder.layer.4.attention.self.query.weight_integer', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.attention.output.output_activation.act_scaling_factor', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias_integer', 'encoder.layer.2.attention.output.output_activation.x_min', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias_integer', 'encoder.layer.2.attention.output.LayerNorm.activation.x_min', 'encoder.layer.9.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.6.output.output_activation.act_scaling_factor', 'encoder.layer.7.output.ln_input_act.act_scaling_factor', 'encoder.layer.0.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.9.attention.output.LayerNorm.shift', 'encoder.layer.4.attention.self.query_activation.x_min', 'encoder.layer.0.attention.self.output_activation.x_min', 'encoder.layer.11.attention.output.LayerNorm.activation.x_max', 'encoder.layer.4.output.ln_input_act.x_min', 'encoder.layer.4.attention.output.output_activation.x_min', 'encoder.layer.4.attention.self.softmax.act.x_max', 'encoder.layer.6.intermediate.dense.weight_integer', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.pre_output_act.x_min', 'encoder.layer.4.attention.self.query.fc_scaling_factor', 'encoder.layer.9.attention.self.value.weight_integer', 'encoder.layer.3.attention.output.output_activation.x_min', 'encoder.layer.4.attention.self.key_activation.x_max', 'encoder.layer.9.output.LayerNorm.activation.x_min', 'encoder.layer.2.pre_output_act.x_min', 'encoder.layer.1.pre_output_act.act_scaling_factor', 'encoder.layer.1.intermediate.output_activation.x_min', 'encoder.layer.3.output.LayerNorm.shift', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.self.output_activation.x_min', 'encoder.layer.6.intermediate.dense.bias_integer', 'encoder.layer.10.attention.self.key.weight_integer', 'encoder.layer.5.attention.output.output_activation.act_scaling_factor', 'encoder.layer.10.pre_intermediate_act.x_max', 'encoder.layer.4.pre_output_act.x_max', 'encoder.layer.5.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.8.attention.output.dense.weight_integer', 'embeddings.LayerNorm.activation.x_min', 'encoder.layer.10.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.7.attention.output.output_activation.act_scaling_factor', 'encoder.layer.0.intermediate.output_activation.x_min', 'encoder.layer.5.attention.output.ln_input_act.x_min', 'encoder.layer.2.attention.self.query_activation.x_min', 'encoder.layer.1.attention.self.query.fc_scaling_factor', 'encoder.layer.2.output.dense.fc_scaling_factor', 'encoder.layer.9.attention.self.value_activation.x_max', 'encoder.layer.8.output.output_activation.x_min', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.attention.self.output_activation.x_max', 'encoder.layer.0.attention.output.LayerNorm.activation.x_max', 'encoder.layer.1.attention.self.value_activation.x_min', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.10.attention.self.key_activation.x_max', 'encoder.layer.10.attention.output.output_activation.x_min', 'encoder.layer.5.attention.self.key_activation.x_min', 'encoder.layer.10.output.ln_input_act.act_scaling_factor', 'encoder.layer.3.output.output_activation.x_min', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.attention.self.output_activation.x_max', 'encoder.layer.1.attention.self.query_activation.act_scaling_factor', 'encoder.layer.1.attention.self.softmax.act.x_max', 'encoder.layer.5.output.dense.bias_integer', 'encoder.layer.6.output.output_activation.x_max', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias_integer', 'encoder.layer.4.output.LayerNorm.activation.x_max', 'encoder.layer.8.pre_output_act.act_scaling_factor', 'encoder.layer.1.attention.output.output_activation.act_scaling_factor', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.8.attention.self.query_activation.x_min', 'encoder.layer.9.attention.self.softmax.act.x_min', 'encoder.layer.1.attention.self.value_activation.act_scaling_factor', 'encoder.layer.8.attention.self.softmax.act.x_max', 'encoder.layer.6.attention.self.output_activation.act_scaling_factor', 'encoder.layer.9.attention.self.query_activation.x_max', 'encoder.layer.9.output.dense.fc_scaling_factor', 'encoder.layer.10.attention.output.ln_input_act.x_max', 'encoder.layer.5.pre_output_act.x_max', 'encoder.layer.0.output.output_activation.act_scaling_factor', 'encoder.layer.9.attention.self.softmax.act.x_max', 'encoder.layer.10.attention.output.output_activation.act_scaling_factor', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.bias_integer', 'encoder.layer.11.intermediate.output_activation.act_scaling_factor', 'encoder.layer.0.attention.self.key.fc_scaling_factor', 'embeddings.position_embeddings.weight_scaling_factor', 'encoder.layer.3.attention.self.output_activation.x_max', 'encoder.layer.6.attention.self.value.fc_scaling_factor', 'encoder.layer.5.attention.self.output_activation.x_min', 'encoder.layer.6.attention.self.output_activation.x_min', 'encoder.layer.1.output.output_activation.x_min', 'encoder.layer.0.pre_output_act.act_scaling_factor', 'encoder.layer.2.intermediate.dense.weight', 'embeddings.position_embeddings.weight_integer', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.pre_output_act.x_max', 'encoder.layer.11.attention.self.query.weight_integer', 'encoder.layer.5.intermediate.dense.bias_integer', 'encoder.layer.2.output.ln_input_act.x_max', 'encoder.layer.10.attention.self.query.weight_integer', 'encoder.layer.1.output.output_activation.act_scaling_factor', 'encoder.layer.1.attention.self.output_activation.x_min', 'encoder.layer.10.attention.output.dense.bias_integer', 'encoder.layer.10.attention.self.value.weight_integer', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias_integer', 'encoder.layer.4.pre_output_act.x_min', 'encoder.layer.2.attention.self.key.fc_scaling_factor', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.fc_scaling_factor', 'encoder.layer.2.attention.self.key.bias_integer', 'encoder.layer.2.attention.self.softmax.act.x_min', 'encoder.layer.4.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.2.attention.output.output_activation.act_scaling_factor', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.shift', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.1.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.7.attention.self.query_activation.x_min', 'encoder.layer.8.output.output_activation.act_scaling_factor', 'encoder.layer.5.attention.self.query.bias_integer', 'encoder.layer.3.output.dense.bias_integer', 'encoder.layer.7.attention.output.ln_input_act.x_min', 'encoder.layer.11.attention.output.LayerNorm.activation.x_min', 'encoder.layer.0.attention.self.value_activation.act_scaling_factor', 'encoder.layer.6.attention.output.dense.bias_integer', 'encoder.layer.10.attention.output.dense.weight_integer', 'encoder.layer.0.attention.output.dense.bias_integer', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.softmax.act.x_max', 'encoder.layer.6.output.LayerNorm.shift', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.output.output_activation.x_max', 'encoder.layer.2.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.attention.self.value.bias_integer', 'encoder.layer.4.attention.self.key.bias_integer', 'encoder.layer.2.attention.output.dense.fc_scaling_factor', 'encoder.layer.10.output.LayerNorm.activation.x_min', 'encoder.layer.2.attention.output.LayerNorm.shift', 'encoder.layer.5.output.output_activation.x_max', 'encoder.layer.6.attention.self.query_activation.x_min', 'encoder.layer.4.attention.self.key_activation.act_scaling_factor', 'encoder.layer.8.attention.self.key.fc_scaling_factor', 'encoder.layer.4.pre_intermediate_act.act_scaling_factor', 'encoder.layer.10.attention.self.key.fc_scaling_factor', 'encoder.layer.10.attention.self.output_activation.x_max', 'encoder.layer.5.intermediate.output_activation.x_max', 'encoder.layer.7.intermediate.output_activation.x_max', 'encoder.layer.7.attention.self.query_activation.act_scaling_factor', 'encoder.layer.11.attention.output.dense.bias_integer', 'encoder.layer.7.attention.self.value_activation.act_scaling_factor', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.shift', 'encoder.layer.2.output.output_activation.x_max', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.output_activation.x_max', 'encoder.layer.4.attention.self.query_activation.x_max', 'encoder.layer.11.attention.self.value_activation.x_min', 'encoder.layer.0.output.dense.bias_integer', 'encoder.layer.8.attention.output.ln_input_act.x_min', 'encoder.layer.1.attention.output.dense.fc_scaling_factor', 'encoder.layer.5.attention.self.output_activation.act_scaling_factor', 'encoder.layer.8.attention.self.query_activation.x_max', 'encoder.layer.3.attention.self.query_activation.act_scaling_factor', 'encoder.layer.0.attention.self.value.bias_integer', 'encoder.layer.3.output.dense.fc_scaling_factor', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.attention.self.query_activation.act_scaling_factor', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.query.fc_scaling_factor', 'encoder.layer.3.pre_output_act.act_scaling_factor', 'encoder.layer.8.attention.self.key_activation.x_max', 'encoder.layer.10.pre_intermediate_act.act_scaling_factor', 'encoder.layer.11.output.ln_input_act.x_min', 'encoder.layer.0.attention.output.output_activation.x_max', 'pooler.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.fc_scaling_factor', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.0.pre_output_act.x_min', 'encoder.layer.6.attention.self.value_activation.act_scaling_factor', 'encoder.layer.10.attention.output.dense.fc_scaling_factor', 'encoder.layer.0.output.LayerNorm.shift', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.self.softmax.act.x_min', 'encoder.layer.0.attention.self.softmax.act.x_min', 'encoder.layer.6.pre_output_act.x_min', 'encoder.layer.10.output.LayerNorm.activation.x_max', 'encoder.layer.4.output.output_activation.act_scaling_factor', 'encoder.layer.10.attention.self.query.fc_scaling_factor', 'embeddings.embeddings_act2.x_min', 'encoder.layer.5.attention.output.output_activation.x_min', 'encoder.layer.5.output.ln_input_act.act_scaling_factor', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.pre_output_act.x_min', 'embeddings.word_embeddings.weight', 'encoder.layer.9.attention.self.key_activation.x_min', 'encoder.layer.10.attention.self.output_activation.act_scaling_factor', 'encoder.layer.8.output.dense.bias_integer', 'encoder.layer.6.attention.output.LayerNorm.activation.x_max', 'encoder.layer.2.attention.self.output_activation.x_min', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.output.ln_input_act.x_max', 'encoder.layer.4.output.dense.bias_integer', 'encoder.layer.9.attention.self.query.weight_integer', 'encoder.layer.9.attention.self.key_activation.x_max', 'encoder.layer.1.pre_intermediate_act.act_scaling_factor', 'encoder.layer.0.attention.self.key.bias_integer', 'encoder.layer.4.output.dense.bias', 'encoder.layer.7.attention.output.output_activation.x_max', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.output.ln_input_act.act_scaling_factor', 'embeddings.output_activation.x_max', 'encoder.layer.3.attention.output.dense.weight_integer', 'encoder.layer.6.intermediate.output_activation.x_max', 'encoder.layer.11.intermediate.output_activation.x_max', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.8.pre_output_act.x_max', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.self.value.weight_integer', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.intermediate.output_activation.act_scaling_factor', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.fc_scaling_factor', 'encoder.layer.2.attention.self.query.bias_integer', 'encoder.layer.7.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.weight_integer', 'encoder.layer.0.pre_intermediate_act.x_min', 'encoder.layer.10.attention.self.query_activation.x_min', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias_integer', 'encoder.layer.8.attention.output.dense.fc_scaling_factor', 'encoder.layer.2.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.10.attention.self.softmax.act.x_max', 'encoder.layer.9.intermediate.dense.weight_integer', 'encoder.layer.1.attention.output.output_activation.x_min', 'encoder.layer.7.output.output_activation.x_min', 'encoder.layer.0.attention.self.output_activation.act_scaling_factor', 'encoder.layer.4.attention.self.softmax.act.x_min', 'encoder.layer.3.attention.output.LayerNorm.activation.x_max', 'encoder.layer.9.attention.output.LayerNorm.activation.x_min', 'encoder.layer.2.intermediate.output_activation.act_scaling_factor', 'encoder.layer.7.attention.self.softmax.act.act_scaling_factor', 'encoder.layer.10.attention.self.output_activation.x_min', 'encoder.layer.1.attention.self.key.fc_scaling_factor', 'encoder.layer.11.attention.output.dense.weight_integer', 'encoder.layer.5.pre_output_act.act_scaling_factor', 'encoder.layer.7.output.LayerNorm.activation.x_max', 'encoder.layer.9.attention.self.output_activation.x_max', 'encoder.layer.10.intermediate.dense.weight_integer', 'encoder.layer.4.attention.output.dense.fc_scaling_factor', 'encoder.layer.3.attention.self.query.weight_integer', 'encoder.layer.11.output.LayerNorm.activation.x_min', 'encoder.layer.2.output.ln_input_act.act_scaling_factor', 'encoder.layer.11.attention.self.value_activation.act_scaling_factor', 'encoder.layer.1.output.output_activation.x_max', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.1.intermediate.output_activation.act_scaling_factor', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.fc_scaling_factor', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.activation.x_min', 'encoder.layer.11.output.output_activation.x_min', 'encoder.layer.5.output.LayerNorm.activation.x_max', 'encoder.layer.10.intermediate.dense.bias_integer', 'encoder.layer.0.attention.output.LayerNorm.activation.x_min', 'encoder.layer.5.attention.self.query_activation.x_min', 'encoder.layer.8.attention.self.value.bias_integer', 'encoder.layer.2.attention.self.key.weight_integer', 'encoder.layer.4.attention.self.key.fc_scaling_factor', 'encoder.layer.4.attention.self.value_activation.x_max', 'encoder.layer.4.output.dense.fc_scaling_factor', 'encoder.layer.5.attention.self.softmax.act.x_max', 'encoder.layer.6.attention.self.query.weight_integer', 'pooler.dense.bias', 'encoder.layer.7.attention.self.value_activation.x_min', 'encoder.layer.8.attention.output.LayerNorm.activation.act_scaling_factor', 'embeddings.token_type_embeddings.weight_integer', 'encoder.layer.7.pre_output_act.act_scaling_factor', 'encoder.layer.9.attention.output.output_activation.x_min', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.output_activation.act_scaling_factor', 'encoder.layer.4.intermediate.dense.bias_integer', 'encoder.layer.8.attention.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight_integer', 'encoder.layer.11.attention.output.LayerNorm.shift', 'encoder.layer.4.attention.output.ln_input_act.x_min', 'encoder.layer.7.attention.self.key.fc_scaling_factor', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.output.dense.weight_integer', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.intermediate.output_activation.x_max', 'encoder.layer.1.output.ln_input_act.act_scaling_factor', 'encoder.layer.4.attention.self.value.weight_integer', 'embeddings.LayerNorm.weight', 'encoder.layer.7.attention.self.key_activation.act_scaling_factor', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.self.value.weight_integer', 'embeddings.embeddings_act2.act_scaling_factor', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.self.key_activation.x_min', 'encoder.layer.8.attention.self.query_activation.act_scaling_factor', 'encoder.layer.8.output.ln_input_act.act_scaling_factor', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.self.query_activation.x_max', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.activation.x_min', 'encoder.layer.9.output.ln_input_act.x_max', 'encoder.layer.0.pre_intermediate_act.x_max', 'encoder.layer.2.attention.output.LayerNorm.activation.x_max', 'encoder.layer.7.attention.self.query_activation.x_max', 'encoder.layer.8.attention.output.LayerNorm.shift', 'encoder.layer.10.attention.self.value.fc_scaling_factor', 'encoder.layer.4.attention.self.output_activation.act_scaling_factor', 'encoder.layer.0.attention.output.output_activation.act_scaling_factor', 'encoder.layer.5.intermediate.output_activation.act_scaling_factor', 'encoder.layer.4.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.8.pre_intermediate_act.act_scaling_factor', 'encoder.layer.1.intermediate.output_activation.x_max', 'encoder.layer.5.attention.self.query.weight_integer', 'encoder.layer.1.attention.output.ln_input_act.x_max', 'encoder.layer.7.output.dense.fc_scaling_factor', 'encoder.layer.11.attention.output.ln_input_act.x_min', 'encoder.layer.0.attention.self.softmax.act.x_max', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.pre_output_act.x_max', 'encoder.layer.7.attention.output.dense.fc_scaling_factor', 'encoder.layer.8.pre_output_act.x_min', 'encoder.layer.8.intermediate.dense.fc_scaling_factor', 'encoder.layer.9.output.LayerNorm.activation.x_max', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight_integer', 'encoder.layer.4.attention.self.key.weight_integer', 'encoder.layer.1.attention.self.key.bias_integer', 'encoder.layer.2.attention.self.value_activation.x_min', 'encoder.layer.0.pre_intermediate_act.act_scaling_factor', 'encoder.layer.4.pre_intermediate_act.x_max', 'encoder.layer.0.attention.output.dense.weight_integer', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.softmax.act.x_min', 'encoder.layer.6.pre_intermediate_act.act_scaling_factor', 'encoder.layer.5.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.10.attention.self.key_activation.act_scaling_factor', 'encoder.layer.7.output.ln_input_act.x_min', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.self.key_activation.act_scaling_factor', 'encoder.layer.10.output.output_activation.x_max', 'encoder.layer.0.output.ln_input_act.x_min', 'encoder.layer.6.attention.self.value.bias_integer', 'encoder.layer.2.attention.self.value.fc_scaling_factor', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.7.attention.self.key_activation.x_min', 'encoder.layer.0.output.dense.fc_scaling_factor', 'encoder.layer.2.attention.self.value_activation.act_scaling_factor', 'encoder.layer.1.attention.self.key_activation.act_scaling_factor', 'encoder.layer.9.output.LayerNorm.activation.act_scaling_factor', 'encoder.layer.11.output.dense.weight_integer', 'encoder.layer.7.attention.self.softmax.act.x_min', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.self.query.fc_scaling_factor', 'encoder.layer.2.attention.self.key_activation.x_min', 'encoder.layer.4.attention.self.query.bias_integer', 'encoder.layer.9.attention.output.output_activation.act_scaling_factor', 'encoder.layer.0.attention.self.key_activation.act_scaling_factor', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.activation.x_min', 'encoder.layer.0.attention.self.value_activation.x_max', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.output.ln_input_act.x_min', 'encoder.layer.2.attention.self.value.bias_integer', 'encoder.layer.4.attention.self.output_activation.x_min', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.5.output.output_activation.act_scaling_factor', 'encoder.layer.11.attention.output.ln_input_act.act_scaling_factor', 'encoder.layer.10.pre_output_act.x_max', 'encoder.layer.9.pre_output_act.act_scaling_factor', 'encoder.layer.6.attention.self.key.fc_scaling_factor', 'encoder.layer.1.pre_intermediate_act.x_min', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.value_activation.x_min', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.pre_output_act.x_min', 'encoder.layer.0.output.dense.weight_integer', 'encoder.layer.0.attention.self.query.bias_integer', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias_integer', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.activation.x_min', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.ln_input_act.x_min', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.output.dense.weight_integer', 'encoder.layer.5.output.LayerNorm.weight', 'embeddings.output_activation.act_scaling_factor', 'encoder.layer.6.output.dense.fc_scaling_factor', 'encoder.layer.4.attention.self.key_activation.x_min', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.pre_intermediate_act.x_max', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.6.attention.self.value_activation.x_max', 'encoder.layer.0.attention.self.query.weight_integer', 'encoder.layer.10.pre_intermediate_act.x_min', 'encoder.layer.11.attention.self.key_activation.x_max', 'encoder.layer.4.pre_intermediate_act.x_min', 'encoder.layer.0.attention.output.dense.fc_scaling_factor', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias_integer', 'encoder.layer.2.pre_intermediate_act.x_min', 'encoder.layer.6.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight_integer', 'encoder.layer.7.intermediate.output_activation.act_scaling_factor', 'encoder.layer.3.attention.self.query_activation.x_min', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.shift', 'encoder.layer.1.output.LayerNorm.shift', 'encoder.layer.7.intermediate.dense.bias_integer', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.2.attention.self.key_activation.act_scaling_factor', 'encoder.layer.2.output.output_activation.act_scaling_factor', 'encoder.layer.1.attention.self.softmax.act.x_min', 'encoder.layer.3.attention.output.dense.fc_scaling_factor', 'encoder.layer.8.intermediate.dense.bias_integer', 'encoder.layer.1.attention.self.query_activation.x_min', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.attention.self.value_activation.x_max', 'encoder.layer.10.intermediate.output_activation.x_min', 'encoder.layer.3.attention.self.value_activation.x_min', 'encoder.layer.2.intermediate.output_activation.x_max', 'encoder.layer.1.pre_output_act.x_max', 'encoder.layer.7.output.dense.bias_integer', 'encoder.layer.0.attention.self.key_activation.x_min', 'encoder.layer.1.attention.output.output_activation.x_max', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.attention.self.value_activation.act_scaling_factor', 'encoder.layer.11.attention.self.softmax.act.x_min', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModel\n","\n","model = AutoModel.from_pretrained(\"NlpHUST/vibert4news-base-cased\")\n","embedding_matrix = model.embeddings.word_embeddings.weight.detach().numpy()\n","# dim = model.embeddings.word_embeddings.embedding_dim\n","# vocab = model.embeddings.word_embeddings.num_embeddings\n","dim = embedding_matrix.shape[1]\n","vocab = embedding_matrix.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"LInk2WC2LpHa"},"source":["## Train "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1687,"status":"ok","timestamp":1679992108874,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2429c3ad-0042-45ce-a545-3fd41f06abe6","id":"zsHCYKm3LpHb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_5 (InputLayer)           [(None, 50)]         0           []                               \n","                                                                                                  \n"," embedding_3 (Embedding)        (None, 50, 768)      47616000    ['input_5[0][0]']                \n","                                                                                                  \n"," spatial_dropout1d_3 (SpatialDr  (None, 50, 768)     0           ['embedding_3[0][0]']            \n"," opout1D)                                                                                         \n","                                                                                                  \n"," bidirectional_4 (Bidirectional  (None, 50, 200)     522000      ['spatial_dropout1d_3[0][0]']    \n"," )                                                                                                \n","                                                                                                  \n"," bidirectional_5 (Bidirectional  (None, 50, 200)     695200      ['spatial_dropout1d_3[0][0]']    \n"," )                                                                                                \n","                                                                                                  \n"," conv1d_4 (Conv1D)              (None, 49, 50)       20050       ['bidirectional_4[0][0]']        \n","                                                                                                  \n"," conv1d_5 (Conv1D)              (None, 49, 50)       20050       ['bidirectional_5[0][0]']        \n","                                                                                                  \n"," global_average_pooling1d_4 (Gl  (None, 50)          0           ['conv1d_4[0][0]']               \n"," obalAveragePooling1D)                                                                            \n","                                                                                                  \n"," global_max_pooling1d_4 (Global  (None, 50)          0           ['conv1d_4[0][0]']               \n"," MaxPooling1D)                                                                                    \n","                                                                                                  \n"," global_average_pooling1d_5 (Gl  (None, 50)          0           ['conv1d_5[0][0]']               \n"," obalAveragePooling1D)                                                                            \n","                                                                                                  \n"," global_max_pooling1d_5 (Global  (None, 50)          0           ['conv1d_5[0][0]']               \n"," MaxPooling1D)                                                                                    \n","                                                                                                  \n"," concatenate_3 (Concatenate)    (None, 200)          0           ['global_average_pooling1d_4[0][0\n","                                                                 ]',                              \n","                                                                  'global_max_pooling1d_4[0][0]', \n","                                                                  'global_average_pooling1d_5[0][0\n","                                                                 ]',                              \n","                                                                  'global_max_pooling1d_5[0][0]'] \n","                                                                                                  \n"," dense_3 (Dense)                (None, 15)           3015        ['concatenate_3[0][0]']          \n","                                                                                                  \n","==================================================================================================\n","Total params: 48,876,315\n","Trainable params: 48,876,315\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["# Bi-GRU-LSTM-CNN\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from keras.models import Sequential, Model\n","\n","from keras.layers import Dense, Concatenate, Bidirectional, GRU, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Dropout, concatenate\n","from keras.layers import Flatten, LSTM, Input\n","from tensorflow.keras.layers import Embedding\n","\n","units = 100\n","\n","input = Input(shape = (max_len,))\n","emb = Embedding(vocab, dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(input)\n","x1 = SpatialDropout1D(0.2)(emb)\n","\n","x = Bidirectional(GRU(units, return_sequences = True))(x1)\n","x = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n","    \n","y = Bidirectional(LSTM(units, return_sequences = True))(x1)\n","y = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n","    \n","avg_pool1 = GlobalAveragePooling1D()(x)\n","max_pool1 = GlobalMaxPooling1D()(x)\n","    \n","avg_pool2 = GlobalAveragePooling1D()(y)\n","max_pool2 = GlobalMaxPooling1D()(y)\n","    \n","    \n","x = Concatenate(axis=-1)([avg_pool1, max_pool1, avg_pool2, max_pool2])\n","out = Dense(15, activation = \"sigmoid\")(x)\n","\n","model = Model(input, out)\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679992132021,"user_tz":-420,"elapsed":23153,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"8d78c997-3272-4737-d430-da4c6c3c911b","id":"JZIMN3oQLpHc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","55/55 [==============================] - 14s 130ms/step - loss: 0.2739 - acc: 0.4164 - val_loss: 0.2147 - val_acc: 0.5512\n","Epoch 2/3\n","55/55 [==============================] - 5s 87ms/step - loss: 0.1999 - acc: 0.6029 - val_loss: 0.1973 - val_acc: 0.5912\n","Epoch 3/3\n","55/55 [==============================] - 4s 75ms/step - loss: 0.1749 - acc: 0.6489 - val_loss: 0.1908 - val_acc: 0.6037\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fa1c862abe0>"]},"metadata":{},"execution_count":76}],"source":["model.fit(train_features[0].astype(np.float32), train_features[1], validation_data=(dev_features[0].astype(np.float32), dev_features[1]), \n","          batch_size=128, epochs=3, verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"IHjeAjAZLpHd"},"source":["## Eval"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":544,"status":"ok","timestamp":1679992132553,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d2b86613-2f4a-4847-fe1d-3b9d0b2ed0d6","id":"nYipthhTLpHe"},"outputs":[{"output_type":"stream","name":"stdout","text":["57/57 [==============================] - 1s 5ms/step\n"]}],"source":["y_test_pred = model.predict(test_features[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kL_zlKuWLpHe"},"outputs":[],"source":["y_test_pred_new = []\n","\n","for y in y_test_pred:\n","    lb = []\n","    for i in range(0, len(y)):\n","        if y[i] >= 0.5:\n","            lb.append(1)\n","        else:\n","            lb.append(0)\n","    y_test_pred_new.append(lb)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1679992133860,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2cdf8c1a-875c-46eb-ccc1-36d2331e3345","id":"KjoeYm-DLpHf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["36.09087301587298"]},"metadata":{},"execution_count":79}],"source":["f1_score(test_features[1], y_test_pred_new)*100"]},{"cell_type":"code","source":["# model.save(\"drive/MyDrive/CODE/ViTHSD/model/model1.h5\")"],"metadata":{"id":"SUWQfwxddQsj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D1HmkAPwtE9v"},"source":["# Model - pre-trained transformers 2"]},{"cell_type":"markdown","metadata":{"id":"oIP0YzCztE9x"},"source":["## Pre-trained Embedding "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11025,"status":"ok","timestamp":1679988699185,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["7c1f4f302a6a474e9050670d68c8ba11","2b6d576932b647d6900c0acfd641f142","6642082be1d04bf3888897afce86b01b","3931bc8af12f4964a19733cedcc98967","96f81abaaaf94567bd101d481e5c8fae","821a8b94f9444615a6bf2c3896370150","f734b707674e493ab059ea93c17228b2","c0a19c87e30e4b639cc814e975bade13","6cabca0fb95644e29177edef8a2d4757","036545b9ca514c43b3d537e24c5b70cf","0bf24bca8242402880c088a9263e35b3","442be819552b4faca8cc2e5c7e77f89c","ca115d4736194289904c84857ef499ea","f7a33fa7938d494482b5ebe03bf127b1","b13700cd4b324f50b63aedfb223974b2","1922ca375d1248ef87d857b2dfc21103","ce309eb0dd2c46b98b0da0367b1fd92a","e3ecc908e46740a1bafd6f215d6cdaa7","9b3ae1a26768402cb8f19e9b098b4770","110c311c938047369bcdf47b18d840a4","eb2c5e311cca4cbca0ceb561e9d35e6b","d86c6e186dc645499a187e1059b3e0a9"]},"outputId":"e0abe555-2005-4a6a-9184-24c808b58c3e","id":"LVDuWBnntE9y"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c1f4f302a6a474e9050670d68c8ba11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"442be819552b4faca8cc2e5c7e77f89c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import AutoModel\n","\n","model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n","embedding_matrix = model.embeddings.word_embeddings.weight.detach().numpy()\n","dim = model.embeddings.word_embeddings.embedding_dim\n","vocab = model.embeddings.word_embeddings.num_embeddings\n","# dim = embedding_matrix.shape[1]\n","# vocab = embedding_matrix.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"x7etYGRHtE9z"},"source":["## Train "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1679988699185,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"048e5057-9347-4b93-c478-3b7433f482c2","id":"7n3hdCF1tE9z"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 100)]        0           []                               \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, 100, 768)     91812096    ['input_3[0][0]']                \n","                                                                                                  \n"," spatial_dropout1d_1 (SpatialDr  (None, 100, 768)    0           ['embedding_1[0][0]']            \n"," opout1D)                                                                                         \n","                                                                                                  \n"," reshape (Reshape)              (None, 100, 768, 1)  0           ['spatial_dropout1d_1[0][0]']    \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 100, 1, 32)   24608       ['reshape[0][0]']                \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 99, 1, 32)    49184       ['reshape[0][0]']                \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 98, 1, 32)    73760       ['reshape[0][0]']                \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 96, 1, 32)    122912      ['reshape[0][0]']                \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 1, 1, 32)     0           ['conv2d[0][0]']                 \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 32)    0           ['conv2d_1[0][0]']               \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 32)    0           ['conv2d_2[0][0]']               \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 32)    0           ['conv2d_3[0][0]']               \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 4, 1, 32)     0           ['max_pooling2d[0][0]',          \n","                                                                  'max_pooling2d_1[0][0]',        \n","                                                                  'max_pooling2d_2[0][0]',        \n","                                                                  'max_pooling2d_3[0][0]']        \n","                                                                                                  \n"," flatten (Flatten)              (None, 128)          0           ['concatenate_1[0][0]']          \n","                                                                                                  \n"," dropout (Dropout)              (None, 128)          0           ['flatten[0][0]']                \n","                                                                                                  \n"," dense_1 (Dense)                (None, 15)           1935        ['dropout[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 92,084,495\n","Trainable params: 272,399\n","Non-trainable params: 91,812,096\n","__________________________________________________________________________________________________\n"]}],"source":["# TextCNN\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from keras.models import Sequential, Model\n","\n","from keras.layers import Dense, Concatenate, Bidirectional, GRU, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Dropout, concatenate\n","from keras.layers import Flatten, LSTM, Input, Reshape, Conv2D, MaxPool2D\n","from tensorflow.keras.layers import Embedding\n","\n","filter_sizes = [1,2,3,5]\n","num_filters = 32\n","\n","input = Input(shape=(max_len,))\n","x = Embedding(vocab, dim, weights=[embedding_matrix], trainable = False)(input)\n","x = SpatialDropout1D(0.4)(x)\n","x = Reshape((max_len, dim, 1))(x)\n","\n","conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], dim), kernel_initializer='normal',\n","                activation='elu')(x)\n","conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], dim), kernel_initializer='normal',\n","                activation='elu')(x)\n","conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], dim), kernel_initializer='normal',\n","                activation='elu')(x)\n","conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], dim), kernel_initializer='normal',\n","                activation='elu')(x)\n","\n","maxpool_0 = MaxPool2D(pool_size=(max_len - filter_sizes[0] + 1, 1))(conv_0)\n","maxpool_1 = MaxPool2D(pool_size=(max_len - filter_sizes[1] + 1, 1))(conv_1)\n","maxpool_2 = MaxPool2D(pool_size=(max_len - filter_sizes[2] + 1, 1))(conv_2)\n","maxpool_3 = MaxPool2D(pool_size=(max_len - filter_sizes[3] + 1, 1))(conv_3)\n","\n","z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n","z = Flatten()(z)\n","z = Dropout(0.1)(z)\n","\n","out = Dense(15, activation = \"sigmoid\")(z)\n","\n","model = Model(input, out)\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679988708298,"user_tz":-420,"elapsed":9121,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"}},"outputId":"98667daf-cad9-45e8-da66-36dbeb00c40a","id":"vMjo_OHxtE90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","55/55 [==============================] - 3s 11ms/step - loss: 0.3270 - acc: 0.3596 - val_loss: 0.2278 - val_acc: 0.5154\n","Epoch 2/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.2333 - acc: 0.4566 - val_loss: 0.2235 - val_acc: 0.5346\n","Epoch 3/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.2299 - acc: 0.4843 - val_loss: 0.2210 - val_acc: 0.5429\n","Epoch 4/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.2260 - acc: 0.5004 - val_loss: 0.2185 - val_acc: 0.5437\n","Epoch 5/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.2216 - acc: 0.5150 - val_loss: 0.2160 - val_acc: 0.5479\n","Epoch 6/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.2175 - acc: 0.5299 - val_loss: 0.2135 - val_acc: 0.5562\n","Epoch 7/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.2129 - acc: 0.5451 - val_loss: 0.2109 - val_acc: 0.5604\n","Epoch 8/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.2095 - acc: 0.5560 - val_loss: 0.2090 - val_acc: 0.5604\n","Epoch 9/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.2050 - acc: 0.5706 - val_loss: 0.2067 - val_acc: 0.5654\n","Epoch 10/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.2013 - acc: 0.5760 - val_loss: 0.2051 - val_acc: 0.5562\n","Epoch 11/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.1973 - acc: 0.5890 - val_loss: 0.2033 - val_acc: 0.5654\n","Epoch 12/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.1935 - acc: 0.5904 - val_loss: 0.2023 - val_acc: 0.5712\n","Epoch 13/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.1899 - acc: 0.6013 - val_loss: 0.2011 - val_acc: 0.5737\n","Epoch 14/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.1868 - acc: 0.6103 - val_loss: 0.2007 - val_acc: 0.5712\n","Epoch 15/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.1827 - acc: 0.6183 - val_loss: 0.1991 - val_acc: 0.5712\n","Epoch 16/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.1790 - acc: 0.6277 - val_loss: 0.1996 - val_acc: 0.5620\n","Epoch 17/20\n","55/55 [==============================] - 0s 7ms/step - loss: 0.1758 - acc: 0.6260 - val_loss: 0.1997 - val_acc: 0.5554\n","Epoch 18/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.1723 - acc: 0.6394 - val_loss: 0.1983 - val_acc: 0.5729\n","Epoch 19/20\n","55/55 [==============================] - 0s 6ms/step - loss: 0.1693 - acc: 0.6466 - val_loss: 0.1995 - val_acc: 0.5679\n","Epoch 20/20\n","55/55 [==============================] - 0s 7ms/step - loss: 0.1653 - acc: 0.6504 - val_loss: 0.1974 - val_acc: 0.5670\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fa1ac3bffa0>"]},"metadata":{},"execution_count":33}],"source":["model.fit(train_features[0].astype(np.float32), train_features[1], validation_data=(dev_features[0].astype(np.float32), dev_features[1]), \n","          batch_size=128, epochs=20, verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"2y3BwbTetE91"},"source":["## Eval"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":946,"status":"ok","timestamp":1679988709239,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab1b8828-1fa5-4142-8d9e-e72de6b3d3ef","id":"XlTYfdtWtE91"},"outputs":[{"output_type":"stream","name":"stdout","text":["57/57 [==============================] - 0s 2ms/step\n"]}],"source":["y_test_pred = model.predict(test_features[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AszqB-80tE91"},"outputs":[],"source":["y_test_pred_new = []\n","\n","for y in y_test_pred:\n","    lb = []\n","    for i in range(0, len(y)):\n","        if y[i] >= 0.5:\n","            lb.append(1)\n","        else:\n","            lb.append(0)\n","    y_test_pred_new.append(lb)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1679988709240,"user":{"displayName":"Sơn Lưu Thanh","userId":"09824077883060402796"},"user_tz":-420},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8bb4693-c665-45cd-ef1f-12c0ab2fcc83","id":"5EEG74GetE91"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["25.778042328042407"]},"metadata":{},"execution_count":36}],"source":["f1_score(test_features[1], y_test_pred_new)*100"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["-NxuE4bujL3h","HIdSiF-20XCz","NnI9Ruf70bDh","QSK00T-SzDf6","kvrZ_gx0LvTB","RvLezpemHj9f","D1HmkAPwtE9v"],"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7c1f4f302a6a474e9050670d68c8ba11":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b6d576932b647d6900c0acfd641f142","IPY_MODEL_6642082be1d04bf3888897afce86b01b","IPY_MODEL_3931bc8af12f4964a19733cedcc98967"],"layout":"IPY_MODEL_96f81abaaaf94567bd101d481e5c8fae"}},"2b6d576932b647d6900c0acfd641f142":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_821a8b94f9444615a6bf2c3896370150","placeholder":"​","style":"IPY_MODEL_f734b707674e493ab059ea93c17228b2","value":"Downloading (…)lve/main/config.json: 100%"}},"6642082be1d04bf3888897afce86b01b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0a19c87e30e4b639cc814e975bade13","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6cabca0fb95644e29177edef8a2d4757","value":625}},"3931bc8af12f4964a19733cedcc98967":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_036545b9ca514c43b3d537e24c5b70cf","placeholder":"​","style":"IPY_MODEL_0bf24bca8242402880c088a9263e35b3","value":" 625/625 [00:00&lt;00:00, 38.2kB/s]"}},"96f81abaaaf94567bd101d481e5c8fae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"821a8b94f9444615a6bf2c3896370150":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f734b707674e493ab059ea93c17228b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0a19c87e30e4b639cc814e975bade13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cabca0fb95644e29177edef8a2d4757":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"036545b9ca514c43b3d537e24c5b70cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bf24bca8242402880c088a9263e35b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"442be819552b4faca8cc2e5c7e77f89c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca115d4736194289904c84857ef499ea","IPY_MODEL_f7a33fa7938d494482b5ebe03bf127b1","IPY_MODEL_b13700cd4b324f50b63aedfb223974b2"],"layout":"IPY_MODEL_1922ca375d1248ef87d857b2dfc21103"}},"ca115d4736194289904c84857ef499ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce309eb0dd2c46b98b0da0367b1fd92a","placeholder":"​","style":"IPY_MODEL_e3ecc908e46740a1bafd6f215d6cdaa7","value":"Downloading pytorch_model.bin: 100%"}},"f7a33fa7938d494482b5ebe03bf127b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b3ae1a26768402cb8f19e9b098b4770","max":714314041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_110c311c938047369bcdf47b18d840a4","value":714314041}},"b13700cd4b324f50b63aedfb223974b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb2c5e311cca4cbca0ceb561e9d35e6b","placeholder":"​","style":"IPY_MODEL_d86c6e186dc645499a187e1059b3e0a9","value":" 714M/714M [00:07&lt;00:00, 93.7MB/s]"}},"1922ca375d1248ef87d857b2dfc21103":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce309eb0dd2c46b98b0da0367b1fd92a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3ecc908e46740a1bafd6f215d6cdaa7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b3ae1a26768402cb8f19e9b098b4770":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"110c311c938047369bcdf47b18d840a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb2c5e311cca4cbca0ceb561e9d35e6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d86c6e186dc645499a187e1059b3e0a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}